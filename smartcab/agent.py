import random
import math
from environment import Agent, Environment
from planner import RoutePlanner
from simulator import Simulator
from environment import TrafficLight


class LearningAgent(Agent):
	""" An agent that learns to drive in the Smartcab world.
		This is the object you will be modifying. """

	def __init__(self, env, learning=False, epsilon=1.0, alpha=0.5, debug=False, a=0.9):
		super(LearningAgent, self).__init__(env)  # Set the agent in the evironment
		self.planner = RoutePlanner(self.env, self)  # Create a route planner
		self.valid_actions = self.env.valid_actions  # The set of valid actions

		# Set parameters of the learning agent
		self.learning = learning  # Whether the agent is expected to learn
		self.Q = dict()  # Create a Q-table which will be a dictionary of tuples
		self.epsilon = epsilon  # Random exploration factor
		self.alpha = alpha  # Learning factor

		self.debug = debug
		self.trial = 0
		self.a = a

	###########
	## TO DO ##
	###########
	# Set any additional class parameters as needed

	def reset(self, destination=None, testing=False):
		""" The reset function is called at the beginning of each trial.
			'testing' is set to True if testing trials are being used
			once training trials have completed. """

		# Select the destination as the new location to route to
		self.planner.route_to(destination)

		###########
		## TO DO ##
		###########
		self.attenuate()
		# Update epsilon using a decay function of your choice
		# Update additional class parameters as needed
		# If 'testing' is True, set epsilon and alpha to 0

		if testing:
			self.epsilon = 0
			self.alpha = 0

		return None

	def attenuate(self):
		self.trial += 1

		# optimized
		# self.epsilon = self.epsilon - 0.05
		self.epsilon = math.pow(self.a, self.trial)
		#		if self.debug:
		print("epsilon reseted: ", self.epsilon, "trial", self.trial)

	def build_state(self):
		""" The build_state function is called when the agent requests data from the 
			environment. The next waypoint, the intersection inputs, and the deadline 
			are all features available to the agent. """

		# Collect data about the environment
		waypoint = self.planner.next_waypoint()  # The next waypoint
		inputs = self.env.sense(self)  # Visual input - intersection light and traffic
		deadline = self.env.get_deadline(self)  # Remaining deadline

		###########
		## TO DO ##
		###########
		# Set 'state' as a tuple of relevant data for the agent

		state = (inputs['light'], inputs['oncoming'], inputs['left'], inputs['right'], waypoint)

		if self.debug:
			print("state", state)
		return state

	def get_maxQ(self, state):
		""" The get_max_Q function is called when the agent is asked to find the
			maximum Q-value of all actions based on the 'state' the smartcab is in. """

		###########
		## TO DO ##
		###########
		# Calculate the maximum Q-value of all actions for a given state

		maxQ = None
		actions = self.Q.get(state)

		maxQvalue = max(actions.values())  # max reward values

		maxQs = [k for k in actions if actions[k] == maxQvalue]  # max reward actions

		if self.debug:
			print("actions", actions)
			print("maxQs", maxQs)
		maxQ = maxQs[0]  # take the first action

		if self.debug:
			print("maxQ:", maxQ)

		return maxQ

	def create_initial_actions(self):
		initial_actions = {}
		#		state = (inputs['light'], inputs['oncoming'], inputs['left'], inputs['right'], waypoint)
		for action in Environment.valid_actions:
			initial_actions[action] = 0.0

		return initial_actions

	def createQ(self, state):
		""" The createQ function is called when a state is generated by the agent. """

		###########
		## TO DO ##
		###########
		# When learning, check if the 'state' is not in the Q-table
		# If it is not, create a new dictionary for that state
		#   Then, for each action available, set the initial Q-value to 0.0

		if state in self.Q:
			return

		self.Q[state] = self.create_initial_actions()
		if self.debug:
			print("created Q", self.Q[state])

	def choose_action(self, state):
		""" The choose_action function is called when the agent is asked to choose
			which action to take, based on the 'state' the smartcab is in. """

		# Set the agent state and default action
		self.state = state
		self.next_waypoint = self.planner.next_waypoint()
		action = None

		###########
		## TO DO ##
		###########
		# When not learning, choose a random action
		# When learning, choose a random action with 'epsilon' probability
		#   Otherwise, choose an action with the highest Q-value for the current state
		if not self.learning:
			action = random.choice(Environment.valid_actions)
		else:
			r = random.random()
			if self.debug:
				print("r", r, "epsilon", self.epsilon, "choose random action....")
			if r < self.epsilon:
				action = random.choice(Environment.valid_actions)
			else:
				action = self.get_maxQ(state=state)

		if self.debug:
			print("action:", action)

		return action

	def learn(self, state, action, reward):
		""" The learn function is called after the agent completes an action and
			receives an award. This function does not consider future rewards 
			when conducting learning. """

		###########
		## TO DO ##
		###########
		# When learning, implement the value iteration update rule
		#   Use only the learning rate 'alpha' (do not use the discount factor 'gamma')
		if self.debug:
			print("state", state, "action", action, "reward", reward)
			print("before learn", self.Q[state][action])
		r = (1 - self.alpha) * self.Q[state][action] + self.alpha * reward
		self.Q[state][action] = r
		if self.debug:
			print("alpha", self.alpha, "reward", reward, "r", r)
			print("after learn", self.Q[state][action])

		return

	def update(self):
		""" The update function is called when a time step is completed in the 
			environment for a given trial. This function will build the agent
			state, choose an action, receive a reward, and learn if enabled. """

		state = self.build_state()  # Get current state
		self.createQ(state)  # Create 'state' in Q-table
		action = self.choose_action(state)  # Choose an action
		reward = self.env.act(self, action)  # Receive a reward
		self.learn(state, action, reward)  # Q-learn

		return


def run():
	""" Driving function for running the simulation. 
		Press ESC to close the simulation, or [SPACE] to pause the simulation. """

	##############
	# Create the environment
	# Flags:
	#   verbose     - set to True to display additional output from the simulation
	#   num_dummies - discrete number of dummy agents in the environment, default is 100
	#   grid_size   - discrete number of intersections (columns, rows), default is (8, 6)
	env = Environment(verbose=False)

	##############
	# Create the driving agent
	# Flags:
	#   learning   - set to True to force the driving agent to use Q-learning
	#    * epsilon - continuous value for the exploration factor, default is 1
	#    * alpha   - continuous value for the learning rate, default is 0.5

	# agent = env.create_agent(LearningAgent, learning=False)
	agent = env.create_agent(LearningAgent, learning=True, debug=False, epsilon=1, alpha=0.6, a=0.999)

	##############
	# Follow the driving agent
	# Flags:
	#   enforce_deadline - set to True to enforce a deadline metric
	env.set_primary_agent(agent, enforce_deadline=True)

	##############
	# Create the simulation
	# Flags:
	#   update_delay - continuous time (in seconds) between actions, default is 2.0 seconds
	#   display      - set to False to disable the GUI if PyGame is enabled
	#   log_metrics  - set to True to log trial and simulation results to /logs
	#   optimized    - set to True to change the default log file name

	# sim = Simulator(env, update_delay=0.001,  display=False, log_metrics=True, optimized=False)
	# optimized
	sim = Simulator(env, update_delay=0.001, display=False, log_metrics=True, optimized=True)

	##############
	# Run the simulator
	# Flags:
	#   tolerance  - epsilon tolerance before beginning testing, default is 0.05
	#   n_test     - discrete number of testing trials to perform, default is 0
	# sim.run(tolerance=0.05, n_test=10)
	# optimized
	sim.run(tolerance=0.0060, n_test=50)


if __name__ == '__main__':
	run()
